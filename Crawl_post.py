import time
import json
import re
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from urllib.parse import urljoin
from bs4 import BeautifulSoup
# ------------------- COOKIE -------------------

def load_cookies(driver, cookie_file):
    try:
        with open(cookie_file, "r", encoding="utf-8") as f:
            cookies = json.load(f)

        driver.get("https://www.facebook.com")
        time.sleep(3)

        for cookie in cookies:
            cookie.pop("storeId", None)
            cookie.pop("id", None)

            if "sameSite" in cookie:
                if cookie["sameSite"].lower() in ["no_restriction", "unspecified"]:
                    cookie["sameSite"] = "None"

            try:
                driver.add_cookie(cookie)
            except Exception as e:
                print(f"[‚ö†Ô∏è] Kh√¥ng th·ªÉ th√™m cookie {cookie.get('name', '')}: {e}")

        print("[‚úÖ] Cookie ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng.")
        driver.refresh()
        time.sleep(5)
        return True
    except Exception as e:
        print(f"[‚ùå] L·ªói khi t·∫£i cookie: {e}")
        return False


def login_with_cookies(driver, post_link, cookie_file="facebook_test.json"):
    print("[INFO] ƒêang ƒëƒÉng nh·∫≠p b·∫±ng cookie...")
    if not load_cookies(driver, cookie_file):
        print("[‚ùå] Cookie kh√¥ng h·ª£p l·ªá ho·∫∑c b·ªã l·ªói.")
        return False

    driver.get(post_link)
    time.sleep(5)
    if "login" in driver.current_url:
        print("[‚ùå] Cookie h·∫øt h·∫°n! C·∫ßn c·∫≠p nh·∫≠t cookie m·ªõi.")
        return False

    print("[‚úÖ] ƒêƒÉng nh·∫≠p th√†nh c√¥ng b·∫±ng cookie!")
    return True

# ------------------- SCROLL -------------------

def scroll_to_bottom(driver, max_scrolls=2):
    try:
        last_height = driver.execute_script("return document.body.scrollHeight")
        total_scrolls = 0

        while True:
            for _ in range(10):
                driver.execute_script(f"window.scrollBy(0, {last_height/10});")
                time.sleep(2)

            time.sleep(3)
            new_height = driver.execute_script("return document.body.scrollHeight")

            if new_height == last_height:
                print("[‚úÖ] ƒê√£ cu·ªôn h·∫øt n·ªôi dung.")
                break

            last_height = new_height
            total_scrolls += 1

            if max_scrolls and total_scrolls >= max_scrolls:
                print("[‚ö†Ô∏è] ƒê√£ ƒë·∫°t gi·ªõi h·∫°n cu·ªôn.")
                break

    except Exception as e:
        print(f"[‚ùå] L·ªói khi cu·ªôn trang: {e}")

# ------------------- SHARE LINK EXTRACTION via DOM -------------------
def extract_post_links_with_hover(driver, target_links: int, scroll_pause=1):
    """
    Tr√≠ch xu·∫•t link b√†i vi·∫øt sau khi hover v√†o ph·∫ßn t·ª≠ span, d·ª´ng khi ƒë·ªß target_links
    """
    # Inject override setAttribute ƒë·ªÉ b·∫Øt c√°c href thay ƒë·ªïi
    driver.execute_script("""
        window.collectedHrefs = [];
        const origSet = Element.prototype.setAttribute;
        Element.prototype.setAttribute = function(name, value) {
          if (this.tagName.toLowerCase() === 'a' && name === 'href') {
            window.collectedHrefs.push(value);
          }
          return origSet.apply(this, arguments);
        };
    """)
    time.sleep(0.5)

    links = []
    scrolls = 0

    while len(links) < target_links:
        scrolls += 1
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(scroll_pause)

        # B∆∞·ªõc 1: Hover v√†o c√°c ph·∫ßn t·ª≠ span c√≥ aria-labelledby
        try:
            span_xpath = "//a[contains(@href, '?')]//span[contains(@aria-labelledby, '¬´')]"
            span_elements = driver.find_elements(By.XPATH, span_xpath)
            
            for span in span_elements:
                try:
                    ActionChains(driver).move_to_element(span).pause(0.3).perform()
                    time.sleep(0.2)
                except:
                    continue
        except Exception as e:
            print(f"[‚ö†Ô∏è] L·ªói khi hover: {str(e)}")

        # B∆∞·ªõc 2: Thu th·∫≠p c√°c href ƒë√£ ƒë∆∞·ª£c k√≠ch ho·∫°t
        raw = driver.execute_script("return window.collectedHrefs") or []

        # B∆∞·ªõc 3: X·ª≠ l√Ω v√† l·ªçc link
        processed_links = set()  # D√πng set ƒë·ªÉ tr√°nh tr√πng l·∫∑p
        for h in raw:
            if not isinstance(h, str):
                continue

            # Chu·∫©n h√≥a link
            if h.startswith("?"):
                h = urljoin("https://www.facebook.com", h)
            h = h.split("?", 1)[0]

            # L·ªçc link b√†i vi·∫øt h·ª£p l·ªá
            if "/groups/" in h and "/posts/" in h:
                processed_links.add(h)

        # Th√™m c√°c link m·ªõi v√†o k·∫øt qu·∫£
        new_links = [link for link in processed_links if link not in links]
        links.extend(new_links)

        # In th√¥ng b√°o cho c√°c link m·ªõi
        for link in new_links:
            print(f"[‚úÖ] Thu th·∫≠p link: {link}")

        # Ki·ªÉm tra ƒë·ªß s·ªë l∆∞·ª£ng
        if len(links) >= target_links:
            print(f"[INFO] ƒê√£ ƒë·ªß {target_links} link sau {scrolls} l·∫ßn scroll")
            break

        # Fail-safe ƒë·ªÉ tr√°nh l·∫∑p v√¥ h·∫°n
        if scrolls >= 100:
            print("[WARNING] ƒê√£ scroll 100 l·∫ßn nh∆∞ng ch∆∞a ƒë·ªß link")
            break

    print(f"[üîó] T·ªïng c·ªông thu ƒë∆∞·ª£c {len(links)} link b√†i vi·∫øt")
    return links[:target_links]  # Tr·∫£ v·ªÅ ƒë√∫ng s·ªë l∆∞·ª£ng y√™u c·∫ßu
# def extract_post_links_via_override(driver,
#                                      max_scrolls=20,
#                                      scroll_pause=1):
#     print("[INFO] Inject override setAttribute ƒë·ªÉ b·∫Øt href‚Ä¶")
#     driver.execute_script("""
#         window.collectedHrefs = [];
#         const origSet = Element.prototype.setAttribute;
#         Element.prototype.setAttribute = function(name, value) {
#           if (this.tagName.toLowerCase() === 'a' && name === 'href') {
#             window.collectedHrefs.push(value);
#           }
#           return origSet.apply(this, arguments);
#         };
#     """)
#     time.sleep(0.2)

#     print("[INFO] B·∫Øt ƒë·∫ßu scroll + hover ƒë·ªÉ Facebook render href‚Ä¶")
#     for i in range(max_scrolls):
#         driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
#         time.sleep(scroll_pause)
#         elems = driver.find_elements(By.XPATH, "//a[@role='link']")
#         for a in elems:
#             try:
#                 ActionChains(driver).move_to_element(a).perform()
#             except:
#                 pass

#     print("[INFO] L·∫•y m·∫£ng raw hrefs t·ª´ page‚Ä¶")
#     raw = driver.execute_script("return window.collectedHrefs") or []

#     # Normalize v√† l·ªçc unique group/post links
#     links = []
#     for h in raw:
#         if not isinstance(h, str):
#             continue
#         # n·∫øu l√† query string th√¨ n·ªëi v·ªõi domain
#         if h.startswith("?"):
#             h = urljoin("https://www.facebook.com", h)
#         # ch·ªâ l·∫•y link c√≥ /groups/ v√† /posts/
#         if "/groups/" in h and "/posts/" in h and h not in links:
#             links.append(h)

#     print(f"[üîó] ƒê√£ thu ƒë∆∞·ª£c {len(links)} link b√†i vi·∫øt:")
#     for link in links:
#         print("  ", link)

#     return links
#usder find
# def extract_share_links(driver):
#     print("[INFO] B·∫Øt ƒë·∫ßu tr√≠ch xu·∫•t c√°c link b√†i vi·∫øt t·ª´ DOM...")
#     links = []
#     pattern = r"https://www\.facebook\.com/(groups|posts|permalink|share|p)/"

#     # T·∫£i l·∫°i DOM sau khi cu·ªôn
#     anchors = driver.find_elements(By.XPATH, "//a[@role='link']")
#     for a in anchors:
#         href = a.get_attribute('href')
#         if href and re.match(pattern, href):
#             if href not in links:
#                 links.append(href)
#                 print(f"[‚úÖ] Ph√°t hi·ªán link: {href}")

#     print(f"[üîó] T·ªïng c·ªông t√¨m ƒë∆∞·ª£c {len(links)} link b√†i vi·∫øt.")
#     return links

# def extract_share_links(driver):
#     print("[INFO] B·∫Øt ƒë·∫ßu ph√¢n t√≠ch DOM v·ªõi BeautifulSoup...")
#     links = []
#     pattern = r"https://www\\.facebook\\.com/(groups|posts|permalink|share|watch|story\\.php|\\?fbid=|\\?story_fbid=)"

#     html = driver.page_source
#     soup = BeautifulSoup(html, "html.parser")

#     for a in soup.find_all("a", href=True):
#         href = a['href']
#         print(f"üîó <a>: {href}")
#         if "/user/" in href or "/media/" in href:
#             continue  # B·ªè qua user v√† media
#         if re.search(pattern, href) and href.startswith("https://www.facebook.com"):
#             if href not in links:
#                 links.append(href)
#                 print(f"[‚úÖ] Link h·ª£p l·ªá: {href}")

#     print(f"[üîó] T·ªïng c·ªông t√¨m ƒë∆∞·ª£c {len(links)} link b√†i vi·∫øt.")
#     return links

# ------------------- SAVE TO EXCEL -------------------

def save_links_to_excel(post_links, filename="post_scrapping.xlsx"):
    new_df = pd.DataFrame(post_links, columns=["Link B√†i Vi·∫øt"])

    try:
        existing_df = pd.read_excel(filename, engine='openpyxl')
        final_df = pd.concat([existing_df, new_df], ignore_index=True)
    except FileNotFoundError:
        final_df = new_df

    with pd.ExcelWriter(filename, engine='openpyxl', mode='w') as writer:
        final_df.to_excel(writer, index=False)

    print(f"[üíæ] ƒê√£ l∆∞u {len(post_links)} link v√†o: {filename}")

# ------------------- MAIN -------------------

def main():
    post_link = input("üîó Nh·∫≠p link nh√≥m Facebook c·∫ßn qu√©t: ")

    options = webdriver.ChromeOptions()
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--headless=new")
    options.add_argument("--lang=vi-VN")
    options.add_experimental_option("useAutomationExtension", False)

    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)

    try:
        if not login_with_cookies(driver, post_link):
            driver.quit()
            return

        print("[üöÄ] B·∫Øt ƒë·∫ßu cu·ªôn trang v√† l·∫•y li√™n k·∫øt...")
        scroll_to_bottom(driver)

        post_links = extract_post_links_with_hover(driver, target_links=15)
        save_links_to_excel(post_links)
    finally:
        driver.quit()
        print("[‚úÖ] ƒê√£ ho√†n t·∫•t to√†n b·ªô qu√° tr√¨nh!" )

if __name__ == "__main__":
    main()
